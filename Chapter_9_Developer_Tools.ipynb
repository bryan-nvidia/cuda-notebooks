{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39e1d2ba-2981-44db-a02c-8a1e68fe5725",
   "metadata": {},
   "source": [
    "# Chapter 9: Developer Tools\n",
    "\n",
    "## CUDA Python Performance\n",
    "\n",
    "In order to achieve optimal performance in CUDA, you must consider several factors:\n",
    "- Localizing memory access in order to minimize memory latency.\n",
    "- Maximizing the number of active threads per multiprocessor to ensure high utilization of your hardware.\n",
    "- Minimization of conditional branching.\n",
    "\n",
    "In order to overcome the bottleneck between CPU and GPU across the PCIe bus, we want to:\n",
    "- Minimize the volume of data transferred.  Transferring data in large batches can minimize the number of data transfer operations.\n",
    "- Organize data in a way that complements the hardware architecture.\n",
    "- Utilize asynchronous transfer features that will allow computation and data transfer to occur simultaneously.  Overlapping data transfers with computation can hide latencies caused by data transfers. \n",
    "\n",
    "## Common Pitfalls\n",
    "The most common mistake is running a CPU-only code on a GPU node. Only codes that have been explicitly written to run on a GPU can take advantage of a GPU. Ensure your codes are using the correct GPU accelerated libraries, drivers, and hardware.\n",
    "\n",
    "**Zero GPU Utilization**\n",
    "Check to make sure your software is GPU enabled.  Only codes that have been explicitly written to use GPUs can take advantage of them.\n",
    "Make sure your software environment is properly configured. In some cases certain libraries must be available for your code to run on GPUs. Check your dependencies, version of CUDA Toolkit, and your software environment requirements.\n",
    " \n",
    "**Low GPU Utilization** (e.g. less than ~15%)\n",
    "Using more GPUs than necessary.  You can find the optimal number of GPUs and CPU-cores by performing a scaling analysis.\n",
    "Check your process’s throughput.  If you are writing output to slow memory, making unnecessary copies, or switching between your CPU and GPU, you may see low utilization.\n",
    "\n",
    "**Memory Errors**\n",
    "Access Violation Errors.  Reading or writing to memory locations that are not allowed or permitted can result in unpredictable behavior and system crashes.\n",
    "Memory Leaks.  When memory is allocated but not correctly deallocated, the application will consume GPU memory resources, but not utilize them.  The allocated memory will not be available for further computation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665a4579-1dce-4ae2-b237-f89c17a26412",
   "metadata": {},
   "source": [
    "## Debugging and Profiling CUDA Python\n",
    "In order to take advantage of the optimizations available through CUDA, debugging and analyzing memory issues is essential to creating accelerated Python applications.  \n",
    "\n",
    "### External Tools\n",
    "**Nsight**\n",
    "\n",
    "NVIDIA Nsight™ Systems is a system-wide performance analysis tool designed to visualize an application’s algorithms, identify the largest opportunities to optimize, and tune to scale efficiently across any quantity or size of CPUs and GPUs, from large servers to our smallest systems-on-a-chip (SoCs).\n",
    "\n",
    "This suite of tools offers an array of interactive as well as command-line tools.  They can detect and provide insight into kernel execution and memory issues.  Ultimately, we need memory usage to align with the GPU hardware preferences.  When these two components are out of alignment, applications may use non-performant access or execution patterns.\n",
    "\n",
    "![NVCube](images/chapter-09/nvidia-developer-tools-1070x400.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252e39da-3c98-40c7-98d4-0494115ab2ef",
   "metadata": {},
   "source": [
    "# Getting Started with NSight Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b618f7db-5a8d-42a6-b666-19c83234fe68",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "\n",
    "How to get setup\n",
    "- Installing tools\n",
    "- Installing nvtx\n",
    "- Installing cuda-python for profiler start/stop apis\n",
    "\n",
    "## Profiling with Nsight Systems\n",
    "\n",
    "[Nsight Systems](https://developer.nvidia.com/nsight-systems) is a platform profiling tool designed to give users a high-level, time-correlated view of the performance activity of their entire platform. This includes CPU, GPU, Memory, Networking, OS and application-level metrics. It helps identify the largest opportunities to optimize, and tune to scale efficiently across all available resources. This tutorial will only scratch the surface of what Nsight Systems is capable of. For full details see the [documentation](https://docs.nvidia.com/nsight-systems/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a956fc6-6066-475d-94df-bb598fad05c7",
   "metadata": {},
   "source": [
    "## Setting up a profile with the Nsight SystemsGUI\n",
    "\n",
    "The first thing to do in the GUI is select the target machine for profiling. This can be the local machine or a remote server. This example uses the local target. To profile a Python workload with Nsight Systems, set the “Command line with arguments:” field to point to the Python interpreter and the Python file to run including and arguments. Make sure the Python executable is in an environment with all the necessary dependencies for the application, for example a Conda shell. For example: “C:\\Users\\myusername\\AppData\\Local\\miniconda3\\python.exe C:\\Users\\myusername\\cupyTests\\cupyProfilingStep1.py\"\n",
    "\n",
    "Also fill in the “Working directory” as appropriate. \n",
    "\n",
    "**Recommended settings/flags**\n",
    "\n",
    "A good initial set of flags for profiling Python include:\n",
    "- Collect CPU context switch trace\n",
    "- Collect CUDA trace\n",
    "- Collect GPU metrics\n",
    "- Python profiling options:\n",
    "- Collect Python backtrace samples\n",
    "\n",
    "You can learn more about all the options [here](https://docs.nvidia.com/nsight-systems/UserGuide/index.html#profiling-from-the-gui)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07940860-6db2-4afa-a8ee-13ab48255e7f",
   "metadata": {},
   "source": [
    "# CuPy Profiling Example\n",
    "\n",
    "In this example, we create two CuPy arrays. Then sort one of them and take the dot product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392f4487-5f04-454e-b6f6-1245d94686e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import cupy as cp\n",
    "\n",
    "\n",
    "def create_array(x, y) :\n",
    "    return cp.random.random((x, y),dtype=cp.float32)\n",
    "\n",
    "def sort_array(a) :\n",
    "    return cp.sort(a)\n",
    "\n",
    "def run_program() :\n",
    "    print(\"init step...\")\n",
    "    arr1 = create_array(10_000, 10_000)\n",
    "    arr2 = create_array(10_000, 10_000)\n",
    "\n",
    "    print(\"sort step...\")\n",
    "    arr1 = sort_array(arr1)\n",
    "\n",
    "    print(\"dot step...\")\n",
    "    arr3 = cp.dot(arr1, arr2)\n",
    "    \n",
    "    print(\"done\")\n",
    "    return\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    run_program()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54919a18-ee89-475f-abef-98bc4a5e64b2",
   "metadata": {},
   "source": [
    "\n",
    "**Step 1 - Profiling a CuPy workload**\n",
    "\n",
    "First, run an initial profile of this CuPy sample using the setup and flags described above. Once the profile completes, zoom in to the active portion of the Python thread. The stuff after is python gunk (more on that later). If you hover over a sample in the Python Backtrace row, you will see the call stack that was currently executing.\n",
    "\n",
    "![cupy1](images/chapter-09/cupy-profiling-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02262227-93ae-4a13-965a-b3cd5e32d10b",
   "metadata": {},
   "source": [
    "CuPy will call CUDA kernels under the hood as it executes. Nsight Systems will automatically detect these. Expand the **CUDA HW** row to see where the kernels are scheduled.\n",
    "\n",
    "![cupy2](images/chapter-09/cupy-profiling-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71a8420-9ed3-40a9-bc12-53f7deb86928",
   "metadata": {},
   "source": [
    "Look at the **GPU Metrics > GPU Active** and **SM Instructions** rows to verify that the GPU is being used. You can hover over a spot in this row to see the % Utilization\n",
    "\n",
    "![cupy3](images/chapter-09/cupy-profiling-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0416a9a1-6758-41bf-abae-06b8fa0a2be3",
   "metadata": {},
   "source": [
    "**Step 2 - Adding nvtx**\n",
    "\n",
    "Nsight Systems can automatically detect CUDA kernels as well as APIs from many other frameworks or libraries. Additionally, the nvtx annotation module gives users the ability to markup their own applications to see personalized trace events and ranges on the timeline. The file <> adds nvtx to the CuPy application, with colored ranges defined around various phases of the workload. Run a profile of this new version to see nvtx on the timeline.\n",
    "\n",
    "The **NVTX** row for the CPU thread of the Python process shows when the CPU is inside one of these ranges. The **NVTX** row under the CUDA HW section shows when these ranges are active on the GPU. Notice that they are not exactly lined up because of GPU execution scheduling. You can also see how the CUDA kernels map to these various nvtx ranges that represent the phases of our workload.\n",
    "\n",
    "In this particular example, we can see in the **GPU Metrics > SM Instructions > Tensor Active** row that the Tensor cores on the GPU are not active while the kernels are running. Tensor cores can add a lot of performance to computation-intensive kernels. The next step will be to get them active. \n",
    "\n",
    "![cupy4](images/chapter-09/cupy-profiling-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e0f27b-10e2-461e-a7c3-aefcd8c515f4",
   "metadata": {},
   "source": [
    "**Step 3 - Enabling Tensor cores** \n",
    "\n",
    "The [CuPy documentation](https://docs.cupy.dev/en/stable/reference/environment.html#envvar-CUPY_TF32) describes how to enable Tensor cores with an environment variable. <file> adds the following line:\n",
    "- os.environ[\"CUPY_TF32\"] = \"1\"\n",
    "\n",
    "Run another Nsight Systems profile to see the activity of the Tensor cores with this version.\n",
    "\n",
    "![cupy5](images/chapter-09/cupy-profiling-5.png)\n",
    "\n",
    "**Notice** that the tensor cores are now being used during the dot product and the runtime of the dot range on the GPU is shorter 312ms ->116ms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38ddda9-e5c5-4de8-8790-f228139421f7",
   "metadata": {},
   "source": [
    "**Step 4 - Using an annotation file** \n",
    "Nsight Systems can also automatically trace specific functions from Python modules, in this case CuPy, with an annotation file. This example points to the file “cupy_annotations.json” which contains:\n",
    "```\n",
    "[\n",
    "    {\n",
    "        \"_comment\": \"CuPy Annotations\",\n",
    "        \n",
    "        \"module\": \"cupy\",\n",
    "   \"color\": \"black\",\n",
    "        \"functions\": [\"random.random\",\"dot\",\"sort\"]\n",
    "    }\n",
    "\n",
    "]\n",
    "```\n",
    "This json object indicates that the functions “random.random”, “dot”, and, “sort” from the module “cupy” should be traced and displayed as a black range on the timeline. Add this file to the “Python Functions trace” field in the configuration as shown below.\n",
    "\n",
    "![cupy6](images/chapter-09/cupy-profiling-6.png)\n",
    "\n",
    "Run another profile to see the automatic tracing. Note that the second random call is much shorter than the first (jit cache?).\n",
    "\n",
    "![cupy7](images/chapter-09/cupy-profiling-7.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5390c8c8-b628-4843-9cec-83fafa37e557",
   "metadata": {},
   "source": [
    "# Numba Profiling Example\n",
    "\n",
    "**Nsight Systems** shows platform-wide profile information and some GPU-specific data, like GPU metrics, but it does not dive deep into the GPU kernels themselves. That’s where Nsight Compute comes in. Nsight Compute does detailed performance analysis of kernels as they run on the GPU. Historically, these have been written in native languages like C, but new technologies like Numba are enabling Python developers to write kernels as well. This section will describe how to profile Numba kernels with Nsight Compute. For an overview of Nsight Compute, check out <>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d7a1bd-8e18-4feb-a914-6edb197bf837",
   "metadata": {},
   "source": [
    "**Setting up a profile with the Nsight Compute GUI**\n",
    "\n",
    "To profile a Numba application with Nsight Compute, open the “Connect” dialog from the GUI. Select the python interpreter binary as the “Application Executable”. Ensure this interpreter runs in the environment with all the necessary dependencies for the application, for example the Conda shell supporting Numba. Then fill in the “Working Directory” field and put your Python file and any additional command line arguments in the “Command Line Arguments” field. This tells Nsight Compute how to launch your workload for profiling.\n",
    "\n",
    "![numba1](images/chapter-09/numba-debug-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a738d9d-be5f-4401-ae07-f434de507ec5",
   "metadata": {},
   "source": [
    "**Recommended settings/flags**\n",
    "\n",
    "Nsight Compute has a lot of options to configure your profile. This guide isn’t designed to cover all of them, but there is a lot of additional information in the <documentation> and <online collateral>. A good starting point for Numba profiling is to choose the “Profile” activity. In the Filter > Kernel Base Name dropdown select “Demangled”. In the Other > Enable CPU Call Stack select Yes and Other > CPU Call Stack Types select All or Python.\n",
    "\n",
    "The “Metrics” tab is where you will choose what performance metrics to collect. The metrics are grouped into sets, and the detailed set is a good starting point. You can learn more about the metrics in the <kernel profiling guide>. After updating these settings, click “Launch” to start the automatic profiling process. Nsight Compute will profile each kernel it encounters via a multi-pass replay mechanism and will report the results once complete.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c38613-a275-408e-9222-cc9f86b5e2fd",
   "metadata": {},
   "source": [
    "### Sample Nsight Compute Profile Walkthrough\n",
    "\n",
    "In this simple example, there is a Numba kernel doing vector addition. It takes in three vectors, adds two together, and returns the sum in the third vector. Notice that the @cuda.jit “decorator?” has the parameter “(lineinfo=True)”. This is important for resolving kernel performance data to lines of source code. With the setup described above, launch a profile to see the performance of the kernel. When the profile completes, the Summary page shows an overview of the kernels profiled. In this example, it’s only one. Expanding the “Demangled Name” column shows that this is the “vecadd” kernel that we wrote with Numba. The Summary has some basic information including the kernel duration and compute and memory throughput. It also lists top performance rules that were triggered and estimated speedups for correcting them. \n",
    "\n",
    "![numba2](images/chapter-09/numba-debug-2.png)\n",
    "\n",
    "Double clicking on the kernel will open the Details page with much more information.\n",
    "\n",
    "The “GPU Speed of Light Throughput” section at the top shows that this kernel has much higher Memory usage than Compute. The Memory Workload Analysis section shows significant traffic to device memory. \n",
    "\n",
    "![numba3](images/chapter-09/numba-debug-3.png)\n",
    "\n",
    "The Compute Workload Analysis section shows the majority of the compute is using the FP64 pipeline. \n",
    "\n",
    "![numba4](images/chapter-09/numba-debug-4.png)\n",
    "\n",
    "The Source Counters section at the bottom shows the source locations with the most stalls and clicking on one opens the Source page. \n",
    "\n",
    "![numba5](images/chapter-09/numba-debug-5.png)\n",
    "\n",
    "Since this was a very simple kernel, most of the stalls are on the addition statement, but with more complex kernels, this level of detail is invaluable. Additionally, the Context page will show the CPU call stack that led to this kernel being executed. \n",
    "\n",
    "![numba6](images/chapter-09/numba-debug-6.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a500f841-445b-4d4d-8cb8-23d55fd626a2",
   "metadata": {},
   "source": [
    "For this example, we did not specify the data type in Numpy which defaulted to FP64. This caused an increase in memory traffic that was unintended. After manually switching to the FP32 datatype and rerunning a profile, we can see that the runtime of the kernel decreased significantly as did the memory traffic. Setting the initial result to the Baseline <link on how to do that> and opening up the new result will automatically compare the two.\n",
    "\n",
    "![Img7](images/chapter-09/numba-debug-7.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ee29d2-f52c-45d0-a3bc-e021a8bb3ad1",
   "metadata": {},
   "source": [
    "Nsight Compute has an abundance of performance data and built-in expertise. Each section on the Details page has detailed information for a particular category of metrics including Guided Analysis rules and descriptions. The best way to learn about all these features is to try it out on your workload and use the documentation and collateral to assist.\n",
    "\n",
    "Sample code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde8e2dd-242c-437c-9d23-c6a2309ed5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda\n",
    "\n",
    "\n",
    "@cuda.jit(lineinfo=True)\n",
    "def vecadd(a, b, c):\n",
    "    tid = cuda.grid(1)\n",
    "    size = len(c)\n",
    "    if tid < size:\n",
    "        c[tid] = a[tid] + b[tid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6c17dd-fff4-4083-b3f6-f2c21fd43c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_program() :\n",
    "\n",
    "\n",
    "    np.random.seed(1)\n",
    "\n",
    "\n",
    "    N = 500000\n",
    "\n",
    "\n",
    "    a = cuda.to_device(np.random.random(N))\n",
    "    b = cuda.to_device(np.random.random(N))\n",
    "    #a = cuda.to_device(np.float32(np.random.random(N)))\n",
    "    #b = cuda.to_device(np.float32(np.random.random(N)))\n",
    "    c = cuda.device_array_like(a)\n",
    "\n",
    "\n",
    "    vecadd.forall(len(a))(a, b, c)\n",
    "    print(c.copy_to_host())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
